{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-D Gridworld\n",
    "\n",
    "This notebook contains an implementation of a Policy Iteration algorithm to estimate and solve a simple Gridworld problem. The policy to be optimized is initially an equiprobable (thus, stochastic) policy, but then converges to a deterministic policy (s.t. $\\pi(a|s)=1$ for one $a$ and $\\pi(a|s)=0$ otherwise. The policy iteration utilizes an iterative policy evaluation. An $\\epsilon$-soft policy is not used.\n",
    "\n",
    "Note that $p(s',r|s,a)=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "act (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct Gridworld\n",
    "    rewards::Array{Int32}\n",
    "end\n",
    "\n",
    "num_states(g::Gridworld) = length(g.rewards)\n",
    "\n",
    "is_terminal_state(g::Gridworld, state::Int32) = state == 1 || state == num_states(g)\n",
    "\n",
    "\"\"\"\n",
    "Performs an action on the gridworld on the given state and returns the recieved reward and the succeeding state.\n",
    "\n",
    "# Arguments\n",
    "- `action`: true for right and false for left\n",
    "\"\"\"\n",
    "function act(g::Gridworld, state::Int64, action::Bool)\n",
    "    reward = 0\n",
    "    if action == 0\n",
    "        new_state = max(1, state - 1)\n",
    "    elseif action == 1\n",
    "        new_state = min(num_states(g), state + 1)\n",
    "    end\n",
    "    if new_state != state\n",
    "        reward = g.rewards[new_state]\n",
    "    end\n",
    "    reward, new_state\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policy_iteration"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solves the gridworld problem using a policy iteration algorithm.\n",
    "\n",
    "# Arguments\n",
    "- `g`: The gridworld to be solved\n",
    "- `gamma`: Reward discounting hyperparameter\n",
    "- `tol`: Small value used to decide whether the policy evaluation converged\n",
    "\n",
    "# Returns\n",
    "- value function as array of length `num_states(g)`\n",
    "- optimal policy as a boolean array of length `num_states(g)`\n",
    "\"\"\"\n",
    "function policy_iteration(g::Gridworld, gamma, tol=1e-2)\n",
    "    delta = tol\n",
    "    policy_stable = false\n",
    "    values = randn(num_states(g))  # Fill value table arbitrarily, but terminal states must be 0\n",
    "    values[1] = 0\n",
    "    values[num_states(g)] = 0\n",
    "    policy = fill(.5, num_states(g))  # P(right), P(left) = 1 - P(right)\n",
    "    \n",
    "    while !policy_stable\n",
    "        # Policy Evaluation\n",
    "        while delta >= tol\n",
    "            delta = 0\n",
    "            for state in 1:num_states(g)\n",
    "                old_value = values[state]\n",
    "                # Only two actions possible, thus it is efficient enough to compute both\n",
    "                right_reward, right_new_state = act(g, state, true)\n",
    "                values[state] = policy[state] * (right_reward + gamma * values[right_new_state])\n",
    "                left_reward, left_new_state = act(g, state, false)\n",
    "                values[state] += (1 - policy[state]) * (left_reward + gamma * values[left_new_state])\n",
    "                delta = max(delta, abs(old_value - values[state]))\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Policy improvement\n",
    "        policy_stable = true\n",
    "        for state in 1:num_states(g)\n",
    "            old_action = policy[state] > 0.5  # old action is right if P(right) > 0.5 and left otherwise\n",
    "            right_reward, right_new_state = act(g, state, true)\n",
    "            left_reward, left_new_state = act(g, state, false)\n",
    "            # if right action is more optimal, set P(right) = 1, otherwise P(right) = 0 => deterministic policy\n",
    "            if right_reward + gamma * values[right_new_state] > left_reward + gamma * values[left_new_state]\n",
    "                policy[state] = 1\n",
    "            else\n",
    "                policy[state] = 0\n",
    "            end\n",
    "            if old_action != policy[state]\n",
    "                policy_stable = false\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    values, policy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gridworld(Int32[10 0 â€¦ 0 -5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridworld = Gridworld([10 0 0 0 0 0 0 0 0 -5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([15.247462870058783, 20.325074323458395, 15.396734688852531, 10.461472881762514, 5.518434468014517, 0.5668925671546345, -4.3937432903252756, -9.36392045261873, -14.343938650542206, -14.333947749503945], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_iteration(gridworld, gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
